var documenterSearchIndex = {"docs":
[{"location":"reference/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"reference/","page":"API Reference","title":"API Reference","text":"Modules = [LocalPoly]","category":"page"},{"location":"reference/#LocalPoly.LocalPoly","page":"API Reference","title":"LocalPoly.LocalPoly","text":"LocalPoly.jl\n\n(Image: Build Status) (Image: ) (Image: )\n\nLocalPoly.jl is a Julia implementation of the local polynomial regression methods outlined in Fan and Gijbels (1996). This package is still experimental, and the API is subject to change.\n\nOverview\n\nThis package provides the function lpreg, which computes the local polynomial regression coefficients for input data of any dimension. Since the local polynomial estimator is much faster over a grid of evenly-spaced data points, performance can be improved by first projecting the data to such a grid using the linear_binning function.\n\nExamples\n\nusing LocalPoly, Random\nRandom.seed!(42)\nx = 2π * rand(1000)\ny = sin.(x) + randn(size(x))/4\ngrid = linear_binning(x, y; nbins=100)\nβ̂ = lpreg(grid)\n\nThe first element of the coefficient vector represents the function estimate at the point of evaluation:\n\njulia> ŷ = first.(β̂)\n100-element Vector{Float64}:\n -0.03070776997429395\n  0.048352477003287916\n  ⋮\n -0.04452583837750935\n -0.04543586963674676\n\nThe estimation points can be recovered by using the gridnodes method:\n\njulia> v = gridnodes(grid)\n100-element Vector{Float64}:\n 0.0005470440483675072\n 0.06395994969485173\n ⋮\n 6.215011797403821\n 6.278424703050305\n\nPlotting the fitted function values against the data:\n\nusing CairoMakie\nf = Figure()\nax = Axis(f[1, 1])\nscatter!(ax, x, y; markersize=2, label=\"Data\")\nlines!(ax, v, sin.(v); color=Cycled(2), label=\"True values\")\nlines!(ax, v, ŷ; color=:tomato, linewidth=3, label=\"Fitted values\")\nLegend(f[2, 1], ax; orientation=:horizontal, framevisible=false)\ncurrent_figure()\n\n(Image: Fit) (Image: Fit)\n\nStandard Errors\n\nThe bias and variance of the estimated coefficients can be estimated by fitting a \"pilot\"  model using a different bandwidth and higher-degree polynomial approximation (see documentation for details):\n\njulia> ci = confint(grid; α=0.05)\n100-element Vector{Tuple{Float64, Float64}}:\n (-0.01564668938703559, 0.2698268678178526)\n (0.04315971509904605, 0.22372303301848356)\n (0.09309771887828536, 0.23823437533995848)\n ⋮\n (-0.24787454088605573, -0.024976639691117714)\n (-0.2363528259834405, 0.04772308836500469)\n (-0.3155131059523961, 0.12284688160121504)\n\nAdding the confidence intervals to the plot:\n\nband!(ax, v, first.(ci), last.(ci); color=(:tomato, 0.3))\ncurrent_figure()\n\n(Image: Fit with confidence interval) (Image: Fit with confidence interval)\n\nDerivative Estimation\n\nOne of the advantages of the local polynomial estimator is its ability to nonparametrically estimate the derivatives of the regression function. To estimate the nu-th derivative of the function, we simply fit a model with degree of at least nu+1.\n\nν = 1\ndegree = ν+1\nh = plugin_bandwidth(grid; ν)\nβ̂1 = lpreg(grid; degree, h)\nŷ′ = [b[ν+1] for b in β̂1]\nci1 = confint(grid; ν, p=degree)\n\nfig, ax, sc = scatter(x, y; color=Cycled(1), markersize=3, label=\"Data\")\nlines!(ax, v, cos.(v); label=\"True values\")\nlines!(ax, v, ŷ′; linewidth=3, label=\"Fitted values\")\nband!(ax, v, first.(ci1), last.(ci1); color=(:tomato, 0.3))\nLegend(fig[2, 1], ax; orientation=:horizontal)\ncurrent_figure()\n\n(Image: Fit derivative curve) (Image: Fit derivative curve)\n\nFits can be improved with more sophisticated bandwidth selection techniques (TBD).\n\nPerformance\n\nSet the number of observations to 100,000 and Y_i = sin(X_i) + varepsilon_i for X_i in 0 2pi. Evaluate the local polynomial estimator at 1,000 points.\n\nusing BenchmarkTools, LocalPoly\nx = 2π * rand(100_000)\ny = sin.(x) + randn(size(x))/10\n@btime grid = linear_binning($x, $y; nbins=1000)\n# 1.072 ms (2 allocations: 15.88 KiB)\n@btime h = plugin_bandwidth($grid)\n# 5.825 μs (14 allocations: 14.23 KiB)\n@btime lpreg($grid; h=$h)\n# 136.371 μs (392 allocations: 35.23 KiB)\n\nR\n\nlibrary(KernSmooth)\nlibrary(microbenchmark)\nx <- 2*pi*runif(100000)\ny <- sin(x) + rnorm(100000)/10\nv <- seq(from = 0, to = 2*pi, length.out = 1000)\nh <- dpill(x, y, gridsize = 1000, range.x = c(0, 2*pi))\nmicrobenchmark(\"KernSmooth\" = locpoly(x, y, bandwidth = h, gridsize = 1000, range.x = c(0, 2*pi)))\n\nOutput:\n\nUnit: milliseconds\n       expr      min       lq     mean   median       uq      max neval\n KernSmooth 2.062024 2.992719 3.506988 3.205222 3.713487 12.05903   100\n\nStata\n\nclear all\nqui set obs 100000\ngen x = 2*3.14159265*runiform()\ngen y = sin(x) + rnormal()/10\nforval i = 1/10 {\n    timer on `i'\n    lpoly y x, n(1000) kernel(epan2) degree(1) nograph\n    timer off `i'\n}\ntimer list\n\nOutput (measured in seconds):\n\n1:     14.59 /        1 =      14.5850\n2:     14.45 /        1 =      14.4500\n3:     14.07 /        1 =      14.0730\n4:     14.31 /        1 =      14.3090\n5:     14.44 /        1 =      14.4440\n6:     14.31 /        1 =      14.3120\n7:     14.06 /        1 =      14.0630\n8:     14.22 /        1 =      14.2160\n9:     14.33 /        1 =      14.3280\n10:     15.00 /        1 =      14.9980\n\nMATLAB\n\nx = rand(100000, 1);\ny = sin(x) + randn(100000, 1)/10;\nv = linspace(min(x), max(x), 1000);\nh = 0.087; % approximate plugin bandwidth\n\nT = 100; % number of benchmark trials to run\ntocs = zeros(T, 1);\nfor i = 1:numel(tocs)\n    tic; lpreg(x, y, v, h); tocs(i) = toc;\nend\nfprintf('mean = %4.3f s\\n std = %4.3f s\\n', mean(tocs), std(tocs));\n\nfunction betas = lpreg(x, y, v, h)\n    X = [ones(size(x)) x];\n    betas = v;\n    for i = 1:numel(v)\n        d = x - v(i);\n        X(:, 2) = d;\n        w = kernelfunc(d/h)/h;\n        beta = inv(X' * (w .* X))*(X' * (w .* y));\n        betas(i) = beta(1);\n    end\n\n    function z = kernelfunc(u)\n        I = abs(u) <= 1;\n        z = zeros(size(u));\n        z(I) = 3*(1-u(I).^2)/4;\n    end\nend\n\nOutput:\n\nmean = 2.739 s\n std = 0.130 s\n\nReferences\n\nFan, J., & Gijbels, I. (1996). Local Polynomial Modelling and its Applications (1st ed.). Chapman & Hall.\nWand, M. P. (1994). Fast Computation of Multivariate Kernel Estimators. Journal of Computational and Graphical Statistics, 3(4), 433–445. https://doi.org/10.2307/1390904\n\n\n\nExports\n\nconfint\ngridnodes\nlinear_binning\nlinear_binning!\nlpreg\nlpreg!\nplugin_bandwidth\n\n\n\n\n\n","category":"module"},{"location":"reference/#LocalPoly.𝐶","page":"API Reference","title":"LocalPoly.𝐶","text":"Dict containing the constant C_nu  p(K) used for the plugin bandwidth\n\n\n\n\n\n","category":"constant"},{"location":"reference/#LocalPoly.allmultiexponents-Tuple{Any, Any}","page":"API Reference","title":"LocalPoly.allmultiexponents","text":"allmultiexponents(N, degree)\n\nAll results of Combinatorics.multiexponents(N, p) for p ≤ degree.\n\nExamples\n\n```julia-repl julia> multiexponents(2, 0) |> collect 1-element Vector{Any}:  [0, 0]\n\njulia> multiexponents(2, 1) |> collect 2-element Vector{Any}:  [1, 0]  [0, 1]\n\njulia> multiexponents(2, 2) |> collect 3-element Vector{Any}:  [2, 0]  [1, 1]  [0, 2]\n\njulia> allmultiexponents(2, 2) 6-element Vector{Tuple{Int64, Int64}}:  (0, 0)  (1, 0)  (0, 1)  (2, 0)  (1, 1)  (0, 2) ````\n\n\n\n\n\n","category":"method"},{"location":"reference/#LocalPoly.gridsteps-Tuple{Any, Any, Any}","page":"API Reference","title":"LocalPoly.gridsteps","text":"gridsteps(g, h, τ)\n\nCompute the number of grid steps for which kernel weights are non-zero. g should be a StepRange.\n\nL = min  τδh M-1 \n\n\n\n\n\n","category":"method"},{"location":"reference/#LocalPoly.outerproduct!-Union{Tuple{N}, Tuple{T}, Tuple{Array{T, N}, Any}} where {T, N}","page":"API Reference","title":"LocalPoly.outerproduct!","text":"outerproduct!(A::Array{T, N}, xs) where {T, N}\n\nCompute the N-dimensional outer product of the vectors xs, modifying A in-place.\n\nExamples\n\njulia> xs = ([0, 1, 2], [1, 2, 3])\n([0, 1, 2], [1, 2, 3])\n\njulia> A = xs[1] * xs[2]'\n3×3 Matrix{Int64}:\n 0  0  0\n 1  2  3\n 2  4  6\n\njulia> fill!(A, 0)\n3×3 Matrix{Int64}:\n 0  0  0\n 0  0  0\n 0  0  0\n\njulia> outerproduct!(A, xs)\n3×3 Matrix{Int64}:\n 0  0  0\n 1  2  3\n 2  4  6\n\n\n\n\n\n","category":"method"},{"location":"reference/#LocalPoly.outerproduct-Union{Tuple{Tuple{Vararg{S, N}}}, Tuple{S}, Tuple{N}} where {N, S}","page":"API Reference","title":"LocalPoly.outerproduct","text":"outerproduct(xs::NTuple{N, T}) where {N, T}\n\nCompute the N-dimensional outer product of the vectors xs\n\nExamples\n\njulia> xs = ([0, 1, 2], [1, 2, 3])\n([0, 1, 2], [1, 2, 3])\n\njulia> xs[1] * xs[2]'\n3×3 Matrix{Int64}:\n 0  0  0\n 1  2  3\n 2  4  6\n\njulia> outerproduct(xs)\n3×3 Matrix{Int64}:\n 0  0  0\n 1  2  3\n 2  4  6\n\n\n\n\n\n","category":"method"},{"location":"reference/#LocalPoly.padκ!-Union{Tuple{N}, Tuple{T}, Tuple{Array{T, N}, Array{T, N}, Array{T, N}}} where {T, N}","page":"API Reference","title":"LocalPoly.padκ!","text":"padκ!(κᶻ::Array{T, N}, κ::Array{T, N}, tmp::Array{T, N}) where {T, N}\n\nZero-pad and shift the convolution kernel, modifying κᶻ in-place.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LocalPoly.padκ-Union{Tuple{N}, Tuple{T}, Tuple{Array{T, N}, Array{T, N}}} where {T, N}","page":"API Reference","title":"LocalPoly.padκ","text":"padκ(κ::Array{T, N}, tmp::Array{T, N}) where {T, N}\n\nZero-pad and shift the convolution kernel.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LocalPoly.plugin_bandwidth-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"API Reference","title":"LocalPoly.plugin_bandwidth","text":"plugin_bandwidth(\n    x::AbstractArray{T<:Real, 1},\n    y::AbstractArray{T<:Real, 1};\n    ν,\n    p,\n    kernel,\n    W\n) -> Any\n\n\nEstimate the rule-of-thumb plugin bandwidth.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LocalPoly.polybasis_grid-Tuple{Any, Any, Any}","page":"API Reference","title":"LocalPoly.polybasis_grid","text":"polybasis_grid(L, δ, p)\n\nConstruct a polynomial basis of L steps of length delta and degree p.\n\nAi j = ((j - (size(A 2)  2 + 1)) * δ)^(i-1)\n\nExamples\n\njulia> polybasis_grid(3, 0.5, 3)\n4×7 Matrix{Float64}:\n  1.0     1.0   1.0     1.0  1.0    1.0  1.0\n -1.5    -1.0  -0.5    -0.0  0.5    1.0  1.5\n  2.25    1.0   0.25    0.0  0.25   1.0  2.25\n -3.375  -1.0  -0.125  -0.0  0.125  1.0  3.375\n\n\n\n\n\n","category":"method"},{"location":"reference/#LocalPoly.togridindex-Tuple{Any, Any, Any}","page":"API Reference","title":"LocalPoly.togridindex","text":"togridindex(x, gmin, δ) = 1 + (x - gmin)/δ\n\nTransform the value x to an index into a grid with minimum gmin and step length δ.\n\nExamples\n\njulia> g = 0:0.1:1\n0.0:0.1:1.0\n\njulia> gmin = minimum(g)\n0.0\n\njulia> δ = step(g)\n0.1\n\njulia> i = togridindex(0.5, gmin, δ)\n6.0\n\njulia> collect(g)[Int(i)] == 0.5\ntrue\n\njulia> fractional_index = togridindex(2^(-1/2), gmin, δ)\n8.071067811865476\n\n\n\n\n\n","category":"method"},{"location":"reference/#LocalPoly.togridindex-Tuple{Any, Any}","page":"API Reference","title":"LocalPoly.togridindex","text":"togridindex(x, g::AbstractRange)\n\nTransform the value x to an index into the range g. This method is ~3 times slower than the direct method, so repeated calls should pre-compute the range minimum and step.\n\nExamples\n\njulia> g = 0:0.1:1\n0.0:0.1:1.0\n\njulia> i = togridindex(0.5, g)\n6.0\n\njulia> collect(g)[Int(i)] == 0.5\ntrue\n\njulia> fractional_index = togridindex(2^(-1/2), gmin, δ)\n8.071067811865476\n\n\n\n\n\n","category":"method"},{"location":"#LocalPoly.jl","page":"Home","title":"LocalPoly.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LocalPoly.jl is a Julia implementation of the local polynomial regression methods outlined in Fan and Gijbels (1996). This package is still experimental, and the API is subject to change.","category":"page"},{"location":"#Local-Polynomial-Regression","page":"Home","title":"Local Polynomial Regression","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Local polynomial regression is a non-parametric estimation technique that can be used to estimate both the conditional mean function and its derivatives.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let (X_i Y_i)_i=1^N be observations (for sake of exposition assumed identically and independently distributed) of the random variables (XY). Let m(x) be the conditional mean function:","category":"page"},{"location":"","page":"Home","title":"Home","text":"m(x) = EYX=x","category":"page"},{"location":"","page":"Home","title":"Home","text":"The conditional mean function m(x) can be approximated in a neighborhood of any point x_0 by a Taylor expansion of degree p:","category":"page"},{"location":"","page":"Home","title":"Home","text":"m(x) approx sum_j=0^p fracm^(p)(x_0)j(x - x_0)^j","category":"page"},{"location":"","page":"Home","title":"Home","text":"This suggests an estimator using the Taylor approximation with weighted data. Let K(cdot) be a valid kernel function, and h the bandwidth or smoothing parameter. Denote K_h(cdot) = K(cdoth)h. The locally weighted sum of squared errors is:","category":"page"},{"location":"","page":"Home","title":"Home","text":"sum_i=1^Nleft Y_i - sum_j=0^p beta_j left(X_i - x_0right)^jright^2 K_h(X_i - x_0)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let widehatbeta  (j=0ldotsp) be the beta minimizing the above expression. Then the nu-th derivative of the conditional mean function evaluated at x_0 is:","category":"page"},{"location":"","page":"Home","title":"Home","text":"widehat m_nu(x_0) = nu widehatbeta_nu","category":"page"},{"location":"#Matrix-Notation","page":"Home","title":"Matrix Notation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The local polynomial estimator can be conveniently expressed using matrix notation. Define the matrices:","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbf X = left( beginmatrix 1  (X_1 - x_0)  cdots  (X_1 - x_0)^p  vdots  vdots   vdots  1  (X_N - x_0)  cdots  (X_N - x_0)^p endmatrix right)","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbf y = left(\n    beginmatrix\n        Y_1 \n        vdots \n        Y_N\n    endmatrix\nright)","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbf W = textdiag left K_h(X_i - x_0) right","category":"page"},{"location":"","page":"Home","title":"Home","text":"Then the weighted sum of squared errors is given by:","category":"page"},{"location":"","page":"Home","title":"Home","text":"left(mathbf y - mathbf X beta right)^prime mathbf W left(mathbf y - mathbf X beta right)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The unique minimizer widehatbeta is then:","category":"page"},{"location":"","page":"Home","title":"Home","text":"widehat beta = left( mathbf X^prime mathbf W mathbf X right)^-1 mathbf X^prime mathbf W mathbf y","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> using LocalPoly\n\njulia> x = 2π * rand(1000);\n\njulia> y = sin.(x) + randn(size(x))/4;\n\njulia> v = range(0, 2π, length=100);\n\njulia> β̂ = lpreg(x, y, v; nbins=100)\n100-element Vector{SVector{2, Float64}}:\n [-0.03070776997429395, 1.2231391275083123]\n [0.048352477003287916, 1.1570071796231207]\n ⋮\n [-0.04452583837750935, 0.7419062295509331]\n [-0.04543586963674676, 0.28981667874915656]","category":"page"},{"location":"","page":"Home","title":"Home","text":"The first element of the coefficient vector represents the function estimate widehat m (v):","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ŷ = first.(β̂)\n100-element Vector{Float64}:\n -0.03070776997429395\n  0.048352477003287916\n  ⋮\n -0.04452583837750935\n -0.04543586963674676","category":"page"},{"location":"","page":"Home","title":"Home","text":"Plotting the fitted function values against the data:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using CairoMakie\nf = Figure()\nax = Axis(f[1, 1])\nscatter!(ax, x, y; markersize=3, label=\"Data\")\nlines!(ax, v, sin.(v); color=:darkgreen, label=\"True values\")\nlines!(ax, v, ŷ; color=:tomato, linewidth=3, label=\"Fitted values\")\nLegend(f[2, 1], ax; orientation=:horizontal, framevisible=false)\ncurrent_figure()","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Fit)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Alternatively, a LPModel object can be constructed to first bin the data before running the regression with the lpreg! method:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> 𝐌 = LPModel(x, y, 1; nbins=100)\nLPModel{Float64}\n        Degree: 1\n  Observations: 1000\n          Bins: 100\n\njulia> β̃ = lpreg!(𝐌, v);\n\njulia> ỹ = first.(β̃);\n\njulia> ỹ == ŷ\ntrue","category":"page"},{"location":"#Standard-Errors","page":"Home","title":"Standard Errors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The conditional variance-covariance matrix can be computed along with the coefficient estimates at each evaluation point by using the keyword argument se=true.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> β̂, V̂ = lpreg(x, y, v; nbins=100, se=true);\n\njulia> V̂[1]\n2×2 SMatrix{2, 2, Float64, 4} with indices SOneTo(2)×SOneTo(2):\n  0.0338293  -0.207557\n -0.207557    1.82918\n\njulia> σ̂ = map(V -> sqrt(V[1, 1]), V̂)\n100-element Vector{Float64}:\n 0.18392746694896067\n 0.12325024737108828\n 0.09069661552755462\n 0.0769932404992409\n ⋮\n 0.08852952186657372\n 0.11884976257937468\n 0.1795255766016528","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can use this to add a confidence interval to the plot:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Distributions\ntᶜ = quantile(TDist(100-2), 1-0.05/2)\nband!(ax, v, ŷ - tᶜ*σ̂, ŷ + tᶜ*σ̂; color=(:tomato, 0.3))\ncurrent_figure()","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Fit with confidence interval)","category":"page"},{"location":"#Performance","page":"Home","title":"Performance","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Set the number of observations N=100  000 and Y_i = sin(X_i) + varepsilon_i for X_i in 0 2pi. Evaluate the local polynomial estimator at 1  000 points.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using BenchmarkTools, LocalPoly\n\njulia> x = 2π * rand(100_000);\n\njulia> y = sin.(x) + randn(size(x))/10;\n\njulia> v = range(minimum(x), maximum(x), length=1000);\n\njulia> @btime lpreg($x, $y, $v);\n  164.850 ms (6506550 allocations: 115.18 MiB)","category":"page"},{"location":"#R","page":"Home","title":"R","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"library(KernSmooth)\nlibrary(microbenchmark)\nx <- 2*pi*runif(100000)\ny <- sin(x) + rnorm(100000)/10\nv <- seq(from = 0, to = 2*pi, length.out = 1000)\nh <- dpill(x, y, gridsize = 1000, range.x = c(0, 2*pi))\nmicrobenchmark(\"Local linear\" = {locpoly(x, y, bandwidth = h, gridsize = 1000, range.x = c(0, 2*pi))})","category":"page"},{"location":"","page":"Home","title":"Home","text":"Output:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Unit: milliseconds\n         expr      min       lq    mean   median       uq      max neval\n Local linear 2.150457 2.262588 2.61474 2.377186 2.598788 5.300715   100","category":"page"},{"location":"#Stata","page":"Home","title":"Stata","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"clear all\nqui set obs 100000\ngen x = 2*3.14159265*runiform()\ngen y = sin(x) + rnormal()/10\nforval i = 1/10 {\n    timer on `i'\n    lpoly y x, n(1000) kernel(epan2) degree(1) nograph\n    timer off `i'\n}\ntimer list","category":"page"},{"location":"","page":"Home","title":"Home","text":"Output (measured in seconds):","category":"page"},{"location":"","page":"Home","title":"Home","text":"1:     14.59 /        1 =      14.5850\n2:     14.45 /        1 =      14.4500\n3:     14.07 /        1 =      14.0730\n4:     14.31 /        1 =      14.3090\n5:     14.44 /        1 =      14.4440\n6:     14.31 /        1 =      14.3120\n7:     14.06 /        1 =      14.0630\n8:     14.22 /        1 =      14.2160\n9:     14.33 /        1 =      14.3280\n10:     15.00 /        1 =      14.9980","category":"page"},{"location":"#MATLAB","page":"Home","title":"MATLAB","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"x = rand(100000, 1);\ny = sin(x) + randn(100000, 1)/10;\nv = linspace(min(x), max(x), 1000);\nh = 0.087; % approximate plugin bandwidth\n\nT = 100; % number of benchmark trials to run\ntocs = zeros(T, 1);\nfor i = 1:numel(tocs)\n    tic; lpreg(x, y, v, h); tocs(i) = toc;\nend\nfprintf('mean = %4.3f s\\n std = %4.3f s\\n', mean(tocs), std(tocs));\n\nfunction betas = lpreg(x, y, v, h)\n    X = [ones(size(x)) x];\n    betas = v;\n    for i = 1:numel(v)\n        d = x - v(i);\n        X(:, 2) = d;\n        w = kernelfunc(d/h)/h;\n        beta = inv(X' * (w .* X))*(X' * (w .* y));\n        betas(i) = beta(1);\n    end\n\n    function z = kernelfunc(u)\n        I = abs(u) <= 1;\n        z = zeros(size(u));\n        z(I) = 3*(1-u(I).^2)/4;\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"Output:","category":"page"},{"location":"","page":"Home","title":"Home","text":"mean = 2.739 s\n std = 0.130 s","category":"page"},{"location":"notation/#DGP","page":"-","title":"DGP","text":"","category":"section"},{"location":"notation/","page":"-","title":"-","text":"Y = m(X) + sigma(X)varepsilon","category":"page"},{"location":"notation/","page":"-","title":"-","text":"Objective function","category":"page"},{"location":"notation/","page":"-","title":"-","text":"min_beta sum_i=1^N left Y_i - sum_j=0^p beta_j left(X_i - x_0right)^j right^2 K_h(X_i - x_0)","category":"page"},{"location":"notation/#Matrices","page":"-","title":"Matrices","text":"","category":"section"},{"location":"notation/","page":"-","title":"-","text":"mathbf X = left(\n    beginmatrix\n        1  (X_1 - x_0)  cdots  (X_1 - x_0)^p \n        vdots  vdots   vdots \n        1  (X_N - x_0)  cdots  (X_N - x_0)^p\n    endmatrix\nright)","category":"page"},{"location":"notation/","page":"-","title":"-","text":"mathbf y = left(\n    beginmatrix\n        Y_1 \n        vdots \n        Y_N\n    endmatrix\nright)","category":"page"},{"location":"notation/","page":"-","title":"-","text":"mathbf W = textdiag left K_h(X_i - x_0) right","category":"page"},{"location":"notation/","page":"-","title":"-","text":"min_beta left(mathbf y - mathbf X beta right)^prime mathbf W left(mathbf y - mathbf X beta right)","category":"page"},{"location":"notation/","page":"-","title":"-","text":"widehat beta = left( mathbf X^prime mathbf W mathbf X right)^-1 mathbf X^prime mathbf W mathbf y","category":"page"},{"location":"notation/#Equivalent-Kernels","page":"-","title":"Equivalent Kernels","text":"","category":"section"},{"location":"notation/","page":"-","title":"-","text":"S_n j = sum_i=1^n K_h (X_i - x_0)(X_i-x_0)^j","category":"page"},{"location":"notation/","page":"-","title":"-","text":"S_n equiv mathbf X^prime mathbf W mathbf X = left( S_nj+l right)_0 leq j l leq p","category":"page"},{"location":"notation/","page":"-","title":"-","text":"widehat beta_nu = e^prime_nu+1 widehatbeta = e^prime_nu+1 S_n^-1 mathbf X^prime mathbf W mathbf y = sum_i=1^n W^n_nu left( fracX_i-x_0h right) Y_i","category":"page"},{"location":"notation/","page":"-","title":"-","text":"W^n_nu (t) = e^prime_nu+1 S_n^-1 left( beginmatrix1  th  vdots  (th)^p endmatrixright) fracK(t)h","category":"page"},{"location":"notation/","page":"-","title":"-","text":"K^*_nu (t) = e^prime_nu+1 S^-1 left( beginmatrix1  t  vdots  t^p endmatrixright) K(t) = left( sum_l=0^p S^nu l t^lright) K(t)","category":"page"},{"location":"notation/","page":"-","title":"-","text":"S^-1 = left( S^jl right)_0 leq j l leq p","category":"page"},{"location":"notation/","page":"-","title":"-","text":"S = (mu_j+l)_0 leq j l leq p","category":"page"},{"location":"notation/","page":"-","title":"-","text":"mu_j = int  u^j K(u)  du","category":"page"}]
}
