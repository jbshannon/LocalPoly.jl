var documenterSearchIndex = {"docs":
[{"location":"reference/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"reference/","page":"API Reference","title":"API Reference","text":"Modules = [LocalPoly]","category":"page"},{"location":"reference/#LocalPoly.LocalPoly","page":"API Reference","title":"LocalPoly.LocalPoly","text":"LocalPoly.jl\n\n(Image: Build Status) (Image: ) (Image: )\n\nLocalPoly.jl is a Julia implementation of the local polynomial regression methods outlined in Fan and Gijbels (1996). This package is still experimental, and the API is subject to change.\n\nOverview\n\nThis package provides the function lpreg, which computes the local polynomial regression coefficients and standard errors at a vector of evaluation knots. This function also (optionally) implements the linear binning method to speed up the computations by reducing the dimensionality of the data. The number of bins is controlled by the keyword argument  nbins (set to 0 for no binning).\n\nExamples\n\nusing LocalPoly, Random\nRandom.seed!(42)\nx = 2œÄ * rand(1000)\ny = sin.(x) + randn(size(x))/4\nv = range(0, 2œÄ, length=100)\nŒ≤ÃÇ = lpreg(x, y, v; nbins=100)\n\nThe first element of the coefficient vector represents the function estimate at the point of evaluation:\n\njulia> yÃÇ = first.(Œ≤ÃÇ)\n100-element Vector{Float64}:\n -0.03070776997429395\n  0.048352477003287916\n  ‚ãÆ\n -0.04452583837750935\n -0.04543586963674676\n\nPlotting the fitted function values against the data:\n\nusing CairoMakie\nf = Figure()\nax = Axis(f[1, 1])\nscatter!(ax, x, y; markersize=3, label=\"Data\")\nlines!(ax, v, sin.(v); color=:darkgreen, label=\"True values\")\nlines!(ax, v, yÃÇ; color=:tomato, linewidth=3, label=\"Fitted values\")\nLegend(f[2, 1], ax; orientation=:horizontal, framevisible=false)\ncurrent_figure()\n\n(Image: Fit) (Image: Fit)\n\nAlternatively, a LPModel object can be constructed to first bin the data before running the regression with the lpreg! method:\n\njulia> ùêå = LPModel(x, y, 1; nbins=100)\nLPModel{Float64}\n        Degree: 1\n  Observations: 1000\n          Bins: 100\n\njulia> Œ≤ÃÉ = lpreg!(ùêå, v);\n\njulia> yÃÉ = first.(Œ≤ÃÉ);\n\njulia> yÃÉ == yÃÇ\ntrue\n\nStandard Errors\n\nThe conditional variance-covariance matrix can be computed along with the coefficient estimates at each evaluation point by using the keyword argument se=true.\n\njulia> Œ≤ÃÇ, VÃÇ = lpreg(x, y, v; nbins=100, se=true);\n\njulia> VÃÇ[1]\n2√ó2 SMatrix{2, 2, Float64, 4} with indices SOneTo(2)√óSOneTo(2):\n  0.0250571  -0.169832\n -0.169832    1.85631\n\njulia> œÉÃÇ = map(V -> sqrt(V[1, 1]), VÃÇ)\n100-element Vector{Float64}:\n 0.15829439002638532\n 0.10565459771497485\n 0.08285173519350204\n ‚ãÆ\n 0.08866937970971286\n 0.11655671525900956\n 0.17517857236448717\n\nWe can use this to add a confidence interval to the plot:\n\nusing Distributions\nt·∂ú = quantile(TDist(100-2), 1-0.05/2)\nband!(ax, v, yÃÇ - t·∂ú*œÉÃÇ, yÃÇ + t·∂ú*œÉÃÇ; color=(:tomato, 0.3))\ncurrent_figure()\n\n(Image: Fit with confidence interval) (Image: Fit with confidence interval)\n\nPerformance\n\nSet the number of observations to 100,000 and Y_i = sin(X_i) + varepsilon_i for X_i in 0 2pi. Evaluate the local polynomial estimator at 1,000 points.\n\nusing BenchmarkTools, LocalPoly\nx = 2œÄ * rand(100_000)\ny = sin.(x) + randn(size(x))/10\nv = range(minimum(x), maximum(x), length=1000)\n@btime h = plugin_bandwidth($x, $y)\n# 2.701 ms (14 allocations: 6.10 MiB)\n@btime lpreg($x, $y, $v; h=$h)\n# 11.204 ms (9563 allocations: 442.02 KiB)\n\nR\n\nlibrary(KernSmooth)\nlibrary(microbenchmark)\nx <- 2*pi*runif(100000)\ny <- sin(x) + rnorm(100000)/10\nv <- seq(from = 0, to = 2*pi, length.out = 1000)\nh <- dpill(x, y, gridsize = 1000, range.x = c(0, 2*pi))\nmicrobenchmark(\"KernSmooth\" = locpoly(x, y, bandwidth = h, gridsize = 1000, range.x = c(0, 2*pi)))\n\nOutput:\n\nUnit: milliseconds\n       expr      min       lq     mean   median       uq      max neval\n KernSmooth 2.062024 2.992719 3.506988 3.205222 3.713487 12.05903   100\n\nStata\n\nclear all\nqui set obs 100000\ngen x = 2*3.14159265*runiform()\ngen y = sin(x) + rnormal()/10\nforval i = 1/10 {\n    timer on `i'\n    lpoly y x, n(1000) kernel(epan2) degree(1) nograph\n    timer off `i'\n}\ntimer list\n\nOutput (measured in seconds):\n\n1:     14.59 /        1 =      14.5850\n2:     14.45 /        1 =      14.4500\n3:     14.07 /        1 =      14.0730\n4:     14.31 /        1 =      14.3090\n5:     14.44 /        1 =      14.4440\n6:     14.31 /        1 =      14.3120\n7:     14.06 /        1 =      14.0630\n8:     14.22 /        1 =      14.2160\n9:     14.33 /        1 =      14.3280\n10:     15.00 /        1 =      14.9980\n\nMATLAB\n\nx = rand(100000, 1);\ny = sin(x) + randn(100000, 1)/10;\nv = linspace(min(x), max(x), 1000);\nh = 0.087; % approximate plugin bandwidth\n\nT = 100; % number of benchmark trials to run\ntocs = zeros(T, 1);\nfor i = 1:numel(tocs)\n    tic; lpreg(x, y, v, h); tocs(i) = toc;\nend\nfprintf('mean = %4.3f s\\n std = %4.3f s\\n', mean(tocs), std(tocs));\n\nfunction betas = lpreg(x, y, v, h)\n    X = [ones(size(x)) x];\n    betas = v;\n    for i = 1:numel(v)\n        d = x - v(i);\n        X(:, 2) = d;\n        w = kernelfunc(d/h)/h;\n        beta = inv(X' * (w .* X))*(X' * (w .* y));\n        betas(i) = beta(1);\n    end\n\n    function z = kernelfunc(u)\n        I = abs(u) <= 1;\n        z = zeros(size(u));\n        z(I) = 3*(1-u(I).^2)/4;\n    end\nend\n\nOutput:\n\nmean = 2.739 s\n std = 0.130 s\n\nReferences\n\nFan, J., & Gijbels, I. (1996). Local Polynomial Modelling and its Applications (1st ed.). Chapman & Hall.\n\n\n\nExports\n\nLPModel\nlpreg\nlpreg!\nplugin_bandwidth\n\n\n\n\n\n","category":"module"},{"location":"reference/#LocalPoly.ùê∂","page":"API Reference","title":"LocalPoly.ùê∂","text":"Dict containing the constant C_nu  p(K) used for the plugin bandwidth\n\n\n\n\n\n","category":"constant"},{"location":"reference/#LocalPoly.LPModel","page":"API Reference","title":"LocalPoly.LPModel","text":"struct LPModel{T<:Real}\n\n\n\nx::Vector{T} where T<:Real\nRaw x data\ny::Vector{T} where T<:Real\nRaw y data\ng::Vector{T} where T<:Real\nBinned x data\nY::Vector{T} where T<:Real\nBinned y data\nc::Vector{T} where T<:Real\nBin weights\nw::Vector{T} where T<:Real\nKernel weight vector\nxÃÇ::AbstractVector{T} where T<:Real\nx data centered at x‚ÇÄ\nW::LinearAlgebra.Diagonal{T, Vector{T}} where T<:Real\nWeighting matrix\nX::Matrix{T} where T<:Real\nPolynomial basis design matrix, centered at x‚ÇÄ\nWX::Matrix{T} where T<:Real\nW * X\nXWX::Any\nWX' * X\nXWY::Vector{T} where T<:Real\nWX' * Y\nŒ£::LinearAlgebra.UniformScaling{T} where T<:Real\nResults vector for XWX\\XWY\nŒ£WX::Matrix{T} where T<:Real\nXWŒ£WX::Matrix{T} where T<:Real\nXWŒ£WXS::Matrix{T} where T<:Real\nVÃÇ::Matrix{T} where T<:Real\n\n\n\n\n\n","category":"type"},{"location":"reference/#LocalPoly.linear_binning-Tuple{Any, Any}","page":"API Reference","title":"LocalPoly.linear_binning","text":"Bins the data x and y using a linear binning algorithm. Returns a tuple (g, Y, c)\n\nlinear_binning(x, y; nbins)\n\n\nOutput\n\ng - gridpoints for the x data\nY - weighted average of y within each bin\nc - weight attached to each gridpoint\n\nExamples\n\njulia> x = 2œÄ * rand(1000);\n\njulia> y = sin.(x) + randn(size(x))/10;\n\njulia> g, Y, c = linear_binning(x, y; nbins=100);\n\njulia> g\n100-element Vector{Float64}:\n 0.007731512161049946\n 0.07102439377241994\n 0.13431727538378993\n ‚ãÆ\n 6.210433910075309\n 6.27372679168668\n\njulia> Y\n100-element Vector{Float64}:\n  0.05587857685614449\n  0.0670877082369906\n  0.0686182710591602\n  ‚ãÆ\n -0.06089998407490756\n  0.007519603457183313\n\njulia> c\n100-element Vector{Float64}:\n  4.7326080715176095\n 14.163103524831035\n  8.148664889642092\n  ‚ãÆ\n 11.265064011130093\n  3.621345671802331\n\n\n\n\n\n","category":"method"},{"location":"reference/#LocalPoly.lpreg!-Union{Tuple{T}, Tuple{LPModel{T}, AbstractVector}} where T","page":"API Reference","title":"LocalPoly.lpreg!","text":"Estimate the local polynomial regression model ùêå at the points in v.\n\nlpreg!(ùêå::LPModel{T}, v::AbstractVector; kernel, h, se) -> Any\n\n\nArguments\n\nùêå::LPModel\nv::AbstractVector\n\nKeyword Arguments\n\nkernel=Val(:Epanechnikov) - kernel function\nh=plugin_bandwidth(x, y, size(ùêå.X, 2)-1, size(ùêå.X, 2); kernel) - bandwidth\nse::Bool=false - flag for whether standard errors should be computed and returned\n\n\n\n\n\n","category":"method"},{"location":"reference/#LocalPoly.lpreg-Tuple{AbstractVector, AbstractVector, AbstractVector}","page":"API Reference","title":"LocalPoly.lpreg","text":"Estimate the local polynomial regression of y on x at the points in v.\n\nlpreg(x::AbstractVector, y::AbstractVector, v::AbstractVector; degree, nbins, kernel, h, se) -> Any\n\n\nArguments\n\nx::AbstractVector\ny::AbstractVector\nv::AbstractVector\n\nKeyword Arguments\n\ndegree::Int=1 - degree of the polynomial approximation\nnbins::Int=floor(Int, length(x)/100) - number of bins to use (0 for no binning)\nkernel=Val(:Epanechnikov) - kernel function\nh=plugin_bandwidth(x, y, size(ùêå.X, 2)-1, size(ùêå.X, 2); kernel) - bandwidth\nse::Bool=false - flag for whether standard errors should be computed and returned\n\n\n\n\n\n","category":"method"},{"location":"reference/#LocalPoly.plugin_bandwidth-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"API Reference","title":"LocalPoly.plugin_bandwidth","text":"plugin_bandwidth(x::AbstractArray{T<:Real, 1}, y::AbstractArray{T<:Real, 1}; ŒΩ, p, kernel) -> Any\n\n\nEstimate the rule-of-thumb plugin bandwidth.\n\n\n\n\n\n","category":"method"},{"location":"#LocalPoly.jl","page":"Home","title":"LocalPoly.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LocalPoly.jl is a Julia implementation of the local polynomial regression methods outlined in Fan and Gijbels (1996). This package is still experimental, and the API is subject to change.","category":"page"},{"location":"#Local-Polynomial-Regression","page":"Home","title":"Local Polynomial Regression","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Local polynomial regression is a non-parametric estimation technique that can be used to estimate both the conditional mean function and its derivatives.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let (X_i Y_i)_i=1^N be observations (for sake of exposition assumed identically and independently distributed) of the random variables (XY). Let m(x) be the conditional mean function:","category":"page"},{"location":"","page":"Home","title":"Home","text":"m(x) = EYX=x","category":"page"},{"location":"","page":"Home","title":"Home","text":"The conditional mean function m(x) can be approximated in a neighborhood of any point x_0 by a Taylor expansion of degree p:","category":"page"},{"location":"","page":"Home","title":"Home","text":"m(x) approx sum_j=0^p fracm^(p)(x_0)j(x - x_0)^j","category":"page"},{"location":"","page":"Home","title":"Home","text":"This suggests an estimator using the Taylor approximation with weighted data. Let K(cdot) be a valid kernel function, and h the bandwidth or smoothing parameter. Denote K_h(cdot) = K(cdoth)h. The locally weighted sum of squared errors is:","category":"page"},{"location":"","page":"Home","title":"Home","text":"sum_i=1^Nleft Y_i - sum_j=0^p beta_j left(X_i - x_0right)^jright^2 K_h(X_i - x_0)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let widehatbeta  (j=0ldotsp) be the beta minimizing the above expression. Then the nu-th derivative of the conditional mean function evaluated at x_0 is:","category":"page"},{"location":"","page":"Home","title":"Home","text":"widehat m_nu(x_0) = nu widehatbeta_nu","category":"page"},{"location":"#Matrix-Notation","page":"Home","title":"Matrix Notation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The local polynomial estimator can be conveniently expressed using matrix notation. Define the matrices:","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbf X = left( beginmatrix 1  (X_1 - x_0)  cdots  (X_1 - x_0)^p  vdots  vdots   vdots  1  (X_N - x_0)  cdots  (X_N - x_0)^p endmatrix right)","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbf y = left(\n    beginmatrix\n        Y_1 \n        vdots \n        Y_N\n    endmatrix\nright)","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbf W = textdiag left K_h(X_i - x_0) right","category":"page"},{"location":"","page":"Home","title":"Home","text":"Then the weighted sum of squared errors is given by:","category":"page"},{"location":"","page":"Home","title":"Home","text":"left(mathbf y - mathbf X beta right)^prime mathbf W left(mathbf y - mathbf X beta right)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The unique minimizer widehatbeta is then:","category":"page"},{"location":"","page":"Home","title":"Home","text":"widehat beta = left( mathbf X^prime mathbf W mathbf X right)^-1 mathbf X^prime mathbf W mathbf y","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> using LocalPoly\n\njulia> x = 2œÄ * rand(1000);\n\njulia> y = sin.(x) + randn(size(x))/4;\n\njulia> v = range(0, 2œÄ, length=100);\n\njulia> Œ≤ÃÇ = lpreg(x, y, v; nbins=100)\n100-element Vector{SVector{2, Float64}}:\n [-0.03070776997429395, 1.2231391275083123]\n [0.048352477003287916, 1.1570071796231207]\n ‚ãÆ\n [-0.04452583837750935, 0.7419062295509331]\n [-0.04543586963674676, 0.28981667874915656]","category":"page"},{"location":"","page":"Home","title":"Home","text":"The first element of the coefficient vector represents the function estimate widehat m (v):","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> yÃÇ = first.(Œ≤ÃÇ)\n100-element Vector{Float64}:\n -0.03070776997429395\n  0.048352477003287916\n  ‚ãÆ\n -0.04452583837750935\n -0.04543586963674676","category":"page"},{"location":"","page":"Home","title":"Home","text":"Plotting the fitted function values against the data:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using CairoMakie\nf = Figure()\nax = Axis(f[1, 1])\nscatter!(ax, x, y; markersize=3, label=\"Data\")\nlines!(ax, v, sin.(v); color=:darkgreen, label=\"True values\")\nlines!(ax, v, yÃÇ; color=:tomato, linewidth=3, label=\"Fitted values\")\nLegend(f[2, 1], ax; orientation=:horizontal, framevisible=false)\ncurrent_figure()","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Fit)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Alternatively, a LPModel object can be constructed to first bin the data before running the regression with the lpreg! method:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ùêå = LPModel(x, y, 1; nbins=100)\nLPModel{Float64}\n        Degree: 1\n  Observations: 1000\n          Bins: 100\n\njulia> Œ≤ÃÉ = lpreg!(ùêå, v);\n\njulia> yÃÉ = first.(Œ≤ÃÉ);\n\njulia> yÃÉ == yÃÇ\ntrue","category":"page"},{"location":"#Standard-Errors","page":"Home","title":"Standard Errors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The conditional variance-covariance matrix can be computed along with the coefficient estimates at each evaluation point by using the keyword argument se=true.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> Œ≤ÃÇ, VÃÇ = lpreg(x, y, v; nbins=100, se=true);\n\njulia> VÃÇ[1]\n2√ó2 SMatrix{2, 2, Float64, 4} with indices SOneTo(2)√óSOneTo(2):\n  0.0338293  -0.207557\n -0.207557    1.82918\n\njulia> œÉÃÇ = map(V -> sqrt(V[1, 1]), VÃÇ)\n100-element Vector{Float64}:\n 0.18392746694896067\n 0.12325024737108828\n 0.09069661552755462\n 0.0769932404992409\n ‚ãÆ\n 0.08852952186657372\n 0.11884976257937468\n 0.1795255766016528","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can use this to add a confidence interval to the plot:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Distributions\nt·∂ú = quantile(TDist(100-2), 1-0.05/2)\nband!(ax, v, yÃÇ - t·∂ú*œÉÃÇ, yÃÇ + t·∂ú*œÉÃÇ; color=(:tomato, 0.3))\ncurrent_figure()","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Fit with confidence interval)","category":"page"},{"location":"#Performance","page":"Home","title":"Performance","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Set the number of observations N=100  000 and Y_i = sin(X_i) + varepsilon_i for X_i in 0 2pi. Evaluate the local polynomial estimator at 1  000 points.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using BenchmarkTools, LocalPoly\n\njulia> x = 2œÄ * rand(100_000);\n\njulia> y = sin.(x) + randn(size(x))/10;\n\njulia> v = range(minimum(x), maximum(x), length=1000);\n\njulia> @btime lpreg($x, $y, $v);\n  164.850 ms (6506550 allocations: 115.18 MiB)","category":"page"},{"location":"#R","page":"Home","title":"R","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"library(KernSmooth)\nlibrary(microbenchmark)\nx <- 2*pi*runif(100000)\ny <- sin(x) + rnorm(100000)/10\nv <- seq(from = 0, to = 2*pi, length.out = 1000)\nh <- dpill(x, y, gridsize = 1000, range.x = c(0, 2*pi))\nmicrobenchmark(\"Local linear\" = {locpoly(x, y, bandwidth = h, gridsize = 1000, range.x = c(0, 2*pi))})","category":"page"},{"location":"","page":"Home","title":"Home","text":"Output:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Unit: milliseconds\n         expr      min       lq    mean   median       uq      max neval\n Local linear 2.150457 2.262588 2.61474 2.377186 2.598788 5.300715   100","category":"page"},{"location":"#Stata","page":"Home","title":"Stata","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"clear all\nqui set obs 100000\ngen x = 2*3.14159265*runiform()\ngen y = sin(x) + rnormal()/10\nforval i = 1/10 {\n    timer on `i'\n    lpoly y x, n(1000) kernel(epan2) degree(1) nograph\n    timer off `i'\n}\ntimer list","category":"page"},{"location":"","page":"Home","title":"Home","text":"Output (measured in seconds):","category":"page"},{"location":"","page":"Home","title":"Home","text":"1:     14.59 /        1 =      14.5850\n2:     14.45 /        1 =      14.4500\n3:     14.07 /        1 =      14.0730\n4:     14.31 /        1 =      14.3090\n5:     14.44 /        1 =      14.4440\n6:     14.31 /        1 =      14.3120\n7:     14.06 /        1 =      14.0630\n8:     14.22 /        1 =      14.2160\n9:     14.33 /        1 =      14.3280\n10:     15.00 /        1 =      14.9980","category":"page"},{"location":"#MATLAB","page":"Home","title":"MATLAB","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"x = rand(100000, 1);\ny = sin(x) + randn(100000, 1)/10;\nv = linspace(min(x), max(x), 1000);\nh = 0.087; % approximate plugin bandwidth\n\nT = 100; % number of benchmark trials to run\ntocs = zeros(T, 1);\nfor i = 1:numel(tocs)\n    tic; lpreg(x, y, v, h); tocs(i) = toc;\nend\nfprintf('mean = %4.3f s\\n std = %4.3f s\\n', mean(tocs), std(tocs));\n\nfunction betas = lpreg(x, y, v, h)\n    X = [ones(size(x)) x];\n    betas = v;\n    for i = 1:numel(v)\n        d = x - v(i);\n        X(:, 2) = d;\n        w = kernelfunc(d/h)/h;\n        beta = inv(X' * (w .* X))*(X' * (w .* y));\n        betas(i) = beta(1);\n    end\n\n    function z = kernelfunc(u)\n        I = abs(u) <= 1;\n        z = zeros(size(u));\n        z(I) = 3*(1-u(I).^2)/4;\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"Output:","category":"page"},{"location":"","page":"Home","title":"Home","text":"mean = 2.739 s\n std = 0.130 s","category":"page"},{"location":"notation/#DGP","page":"-","title":"DGP","text":"","category":"section"},{"location":"notation/","page":"-","title":"-","text":"Y = m(X) + sigma(X)varepsilon","category":"page"},{"location":"notation/","page":"-","title":"-","text":"Objective function","category":"page"},{"location":"notation/","page":"-","title":"-","text":"min_beta sum_i=1^N left Y_i - sum_j=0^p beta_j left(X_i - x_0right)^j right^2 K_h(X_i - x_0)","category":"page"},{"location":"notation/#Matrices","page":"-","title":"Matrices","text":"","category":"section"},{"location":"notation/","page":"-","title":"-","text":"mathbf X = left(\n    beginmatrix\n        1  (X_1 - x_0)  cdots  (X_1 - x_0)^p \n        vdots  vdots   vdots \n        1  (X_N - x_0)  cdots  (X_N - x_0)^p\n    endmatrix\nright)","category":"page"},{"location":"notation/","page":"-","title":"-","text":"mathbf y = left(\n    beginmatrix\n        Y_1 \n        vdots \n        Y_N\n    endmatrix\nright)","category":"page"},{"location":"notation/","page":"-","title":"-","text":"mathbf W = textdiag left K_h(X_i - x_0) right","category":"page"},{"location":"notation/","page":"-","title":"-","text":"min_beta left(mathbf y - mathbf X beta right)^prime mathbf W left(mathbf y - mathbf X beta right)","category":"page"},{"location":"notation/","page":"-","title":"-","text":"widehat beta = left( mathbf X^prime mathbf W mathbf X right)^-1 mathbf X^prime mathbf W mathbf y","category":"page"},{"location":"notation/#Equivalent-Kernels","page":"-","title":"Equivalent Kernels","text":"","category":"section"},{"location":"notation/","page":"-","title":"-","text":"S_n j = sum_i=1^n K_h (X_i - x_0)(X_i-x_0)^j","category":"page"},{"location":"notation/","page":"-","title":"-","text":"S_n equiv mathbf X^prime mathbf W mathbf X = left( S_nj+l right)_0 leq j l leq p","category":"page"},{"location":"notation/","page":"-","title":"-","text":"widehat beta_nu = e^prime_nu+1 widehatbeta = e^prime_nu+1 S_n^-1 mathbf X^prime mathbf W mathbf y = sum_i=1^n W^n_nu left( fracX_i-x_0h right) Y_i","category":"page"},{"location":"notation/","page":"-","title":"-","text":"W^n_nu (t) = e^prime_nu+1 S_n^-1 left( beginmatrix1  th  vdots  (th)^p endmatrixright) fracK(t)h","category":"page"},{"location":"notation/","page":"-","title":"-","text":"K^*_nu (t) = e^prime_nu+1 S^-1 left( beginmatrix1  t  vdots  t^p endmatrixright) K(t) = left( sum_l=0^p S^nu l t^lright) K(t)","category":"page"},{"location":"notation/","page":"-","title":"-","text":"S^-1 = left( S^jl right)_0 leq j l leq p","category":"page"},{"location":"notation/","page":"-","title":"-","text":"S = (mu_j+l)_0 leq j l leq p","category":"page"},{"location":"notation/","page":"-","title":"-","text":"mu_j = int  u^j K(u)  du","category":"page"}]
}
